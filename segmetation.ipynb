{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'arabidopsis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(path):\n",
    "    images = glob(os.path.join(path,'images/*'))\n",
    "    masks = glob(os.path.join(path,'masks/*'))\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = loader(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (128, 128))\n",
    "    x = x / 255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (128, 128))\n",
    "    x = x / 255.0\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def preprocess(x, y):\n",
    "    def f(x, y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    images, masks = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
    "    images.set_shape([128, 128, 3])\n",
    "    masks.set_shape([128, 128, 1])\n",
    "\n",
    "    return images, masks\n",
    "\n",
    "def tf_dataset(x, y, batch=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 16 - Masks: 16\n"
     ]
    }
   ],
   "source": [
    "ath = 'arabidopsis/'\n",
    "images, masks = loader(path)\n",
    "print(f\"Images: {len(images)} - Masks: {len(masks)}\")\n",
    "\n",
    "dataset = tf_dataset(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "   # Conv2D then ReLU activation\n",
    "   x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "   return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "   f = double_conv_block(x, n_filters)\n",
    "   p = layers.MaxPool2D(2)(f)\n",
    "   p = layers.Dropout(0.3)(p)\n",
    "   return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "   # upsample\n",
    "   x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "   # concatenate\n",
    "   x = layers.concatenate([x, conv_features])\n",
    "   # dropout\n",
    "   x = layers.Dropout(0.3)(x)\n",
    "   # Conv2D twice with ReLU activation\n",
    "   x = double_conv_block(x, n_filters)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "   inputs = layers.Input(shape=(128,128,3))\n",
    "\n",
    "   # encoder: contracting path - downsample\n",
    "   # 1 - downsample\n",
    "   f1, p1 = downsample_block(inputs, 64)\n",
    "   # 2 - downsample\n",
    "   f2, p2 = downsample_block(p1, 128)\n",
    "   # 3 - downsample\n",
    "   f3, p3 = downsample_block(p2, 256)\n",
    "   # 4 - downsample\n",
    "   f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "   # 5 - bottleneck\n",
    "   bottleneck = double_conv_block(p4, 1024)\n",
    "\n",
    "   # decoder: expanding path - upsample\n",
    "   # 6 - upsample\n",
    "   u6 = upsample_block(bottleneck, f4, 512)\n",
    "   # 7 - upsample\n",
    "   u7 = upsample_block(u6, f3, 256)\n",
    "   # 8 - upsample\n",
    "   u8 = upsample_block(u7, f2, 128)\n",
    "   # 9 - upsample\n",
    "   u9 = upsample_block(u8, f1, 64)\n",
    "\n",
    "   # outputs\n",
    "   outputs = layers.Conv2D(3, 1, padding=\"same\", activation = \"softmax\")(u9)\n",
    "\n",
    "   # unet model with Keras Functional API\n",
    "   unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "   return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = build_unet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer='adam',\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=\"accuracy\",run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DatasetV2.batch of <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None))>>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9095 - accuracy: 0.7091\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0000e+00 - accuracy: 0.9423\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0000e+00 - accuracy: 0.9446\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0000e+00 - accuracy: 0.9574\n"
     ]
    }
   ],
   "source": [
    "model_history = unet_model.fit(dataset,\n",
    "                              epochs=4,\n",
    "                              steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 880ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img_path = 'AAE_0003.JPG'\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.\n",
    "\n",
    "pred_mask = unet_model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c8adf130>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred_mask[1,:,:,:]*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 128, 128, 3)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mask[:,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
